{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DiploDatos 2018 / Aprendizaje no supervizado / Clustering Demo*\n",
    "\n",
    "# Aplicación de técnicas de *clustering* a documentos de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objetivos:**\n",
    "\n",
    "En este ejemplo mostraremos cómo utilizar técnicas de clustering para aprender la estructura subyacente de un conjunto de documentos de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "from sklearn import feature_extraction\n",
    "import mpld3\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATOS: Top 100 Greatest Movies of All Time (The Ultimate List), by ChrisWalczyk55\n",
    "\n",
    "https://www.imdb.com/list/ls055592025/\n",
    "\n",
    "El problema consiste en agrupar un conjunto de películas en base a sus críticas en inglés, \n",
    "usando para ello procesamiento del texto\n",
    "\n",
    "\n",
    "Lo primero que haremos es leer los datos, disponibles en:\n",
    "https://github.com/brandomr/document_cluster.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lectura de los titulos\n",
    "\n",
    "with open(\"data/document_cluster/title_list.txt\") as file:\n",
    "    titles = [line.strip() for line in file]\n",
    "    \n",
    "# Lectura de las criticas\n",
    "\n",
    "synopses = []\n",
    "with open(\"data/document_cluster/synopses_list_wiki.txt\") as file:\n",
    "    i = True\n",
    "    l = ' '\n",
    "    for line in file:            \n",
    "        if 'BREAKS HERE' in line:\n",
    "            synopses.append(l) # append the previously collected lines\n",
    "            l = ' '       \n",
    "        l = l + line.decode('utf-8').strip()\n",
    "        \n",
    "# Lectura de los generos\n",
    "\n",
    "with open(\"data/document_cluster/genres_list.txt\") as file:\n",
    "    genres = [line.strip() for line in file]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis del texto\n",
    "\n",
    "\n",
    "Para analizar el texto debemos estudiar la frecuencia de las palabras, es decir, separar el texto en unidades sintácticas o *tokens*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['computer', 'science', 'is', 'no', 'more', 'about', 'computers', 'than', 'astronomy', 'is', 'about', 'telescopes', 'edsger', 'dijkstra']\n"
     ]
    }
   ],
   "source": [
    "# e.g.:\n",
    "from nltk.tokenize import word_tokenize\n",
    "text = \"Computer science is no more about computers than astronomy is about telescopes. Edsger Dijkstra\"\n",
    "tokens = tokenize_only(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalvocab_tokenized = []\n",
    "\n",
    "for i in synopses:\n",
    "    allwords_tokenized = tokenize_only(i)\n",
    "    totalvocab_tokenized.extend(allwords_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hay en total 164243 tokens \n",
      "\n",
      "[u'plot', u'edit', u'edit', u'edit', u'on', u'the', u'day', u'of', u'his', u'only', u'daughter', u\"'s\", u'wedding', u'vito', u'corleone', u'hears', u'requests', u'in', u'his', u'role', u'as', u'the', u'godfather', u'the', u'don', u'of', u'a', u'new', u'york', u'crime', u'family', u'vito', u\"'s\", u'youngest', u'son', u'michael', u'in', u'a', u'marine', u'corps', u'uniform', u'introduces', u'his', u'girlfriend', u'kay', u'adams', u'to', u'his', u'family', u'at']\n",
      "there are 164243 items in vocab_frame\n"
     ]
    }
   ],
   "source": [
    "print('Hay en total ' + str(len(totalvocab_tokenized)) + ' tokens \\n')\n",
    "len(totalvocab_tokenized)\n",
    "print (totalvocab_tokenized[0:50])\n",
    "\n",
    "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_tokenized)\n",
    "print('there are ' + str(vocab_frame.shape[0]) + ' items in vocab_frame')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 143)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#define vectorizer parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n",
    "                                 min_df=0.2, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenize_only, ngram_range=(1,3))\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(synopses) #fit the vectorizer to synopses\n",
    "print(tfidf_matrix.shape)\n",
    "\n",
    "terms = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buscar clusters con Kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero tenemos que hacer el *embeding*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 5\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "km.fit(tfidf_matrix)\n",
    "\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 3, 3, 2, 4, 4, 1, 0, 1, 4, 3, 0, 4, 4, 2, 1, 4, 0, 4, 1, 1, 2, 4, 4, 3, 4, 3, 1, 2, 3, 1, 3, 2, 3, 2, 2, 3, 3, 3, 2, 0, 2, 1, 4, 1, 1, 0, 3, 3, 3, 1, 2, 3, 3, 4, 3, 3, 3, 0, 0, 1, 3, 3, 4, 1, 0, 4, 2, 4, 1, 1, 2, 1, 1, 2, 1, 4, 0, 0, 2, 2, 3, 1, 2, 1, 4, 4, 3, 3, 2, 4, 4, 2, 4, 4, 4, 4, 4, 4, 0]\n",
      "El cluster 0 tiene 12 elementos\n",
      "El cluster 1 tiene 20 elementos\n",
      "El cluster 2 tiene 18 elementos\n",
      "El cluster 3 tiene 24 elementos\n",
      "El cluster 4 tiene 26 elementos\n"
     ]
    }
   ],
   "source": [
    "print (clusters)\n",
    "\n",
    "# Recuento del número de elementos en cada cluster\n",
    "for i in range(num_clusters):\n",
    "    print ('El cluster %i tiene %i elementos' % (i, clusters.count(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "films = { 'title': titles, 'synopsis': synopses, 'cluster': clusters, 'genre': genres }\n",
    "frame = pd.DataFrame(films, index = [clusters] , columns = ['title', 'genre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>[u' Crime', u' Drama']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Schindler's List</td>\n",
       "      <td>[u' Biography', u' Drama', u' History']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Raging Bull</td>\n",
       "      <td>[u' Biography', u' Drama', u' Sport']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Casablanca</td>\n",
       "      <td>[u' Drama', u' Romance', u' War']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>One Flew Over the Cuckoo's Nest</td>\n",
       "      <td>[u' Drama']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gone with the Wind</td>\n",
       "      <td>[u' Drama', u' Romance', u' War']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Citizen Kane</td>\n",
       "      <td>[u' Drama', u' Mystery']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Wizard of Oz</td>\n",
       "      <td>[u' Adventure', u' Family', u' Fantasy', u' Mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Titanic</td>\n",
       "      <td>[u' Drama', u' Romance']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             title  \\\n",
       "3         The Shawshank Redemption   \n",
       "3                 Schindler's List   \n",
       "2                      Raging Bull   \n",
       "4                       Casablanca   \n",
       "4  One Flew Over the Cuckoo's Nest   \n",
       "1               Gone with the Wind   \n",
       "0                     Citizen Kane   \n",
       "1                 The Wizard of Oz   \n",
       "4                          Titanic   \n",
       "\n",
       "                                               genre  \n",
       "3                             [u' Crime', u' Drama']  \n",
       "3            [u' Biography', u' Drama', u' History']  \n",
       "2              [u' Biography', u' Drama', u' Sport']  \n",
       "4                  [u' Drama', u' Romance', u' War']  \n",
       "4                                        [u' Drama']  \n",
       "1                  [u' Drama', u' Romance', u' War']  \n",
       "0                           [u' Drama', u' Mystery']  \n",
       "1  [u' Adventure', u' Family', u' Fantasy', u' Mu...  \n",
       "4                           [u' Drama', u' Romance']  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gone with the Wind</td>\n",
       "      <td>[u' Drama', u' Romance', u' War']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Wizard of Oz</td>\n",
       "      <td>[u' Adventure', u' Family', u' Fantasy', u' Mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>On the Waterfront</td>\n",
       "      <td>[u' Crime', u' Drama']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Star Wars</td>\n",
       "      <td>[u' Action', u' Adventure', u' Fantasy', u' Sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E.T. the Extra-Terrestrial</td>\n",
       "      <td>[u' Adventure', u' Family', u' Sci-Fi']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Some Like It Hot</td>\n",
       "      <td>[u' Comedy']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amadeus</td>\n",
       "      <td>[u' Biography', u' Drama', u' Music']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>To Kill a Mockingbird</td>\n",
       "      <td>[u' Drama']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Best Years of Our Lives</td>\n",
       "      <td>[u' Drama', u' Romance', u' War']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My Fair Lady</td>\n",
       "      <td>[u' Drama', u' Family', u' Musical', u' Romance']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Braveheart</td>\n",
       "      <td>[u' Action', u' Biography', u' Drama', u' Hist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Exorcist</td>\n",
       "      <td>[u' Horror']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>City Lights</td>\n",
       "      <td>[u' Comedy', u' Drama', u' Romance']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mr. Smith Goes to Washington</td>\n",
       "      <td>[u' Drama']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rain Man</td>\n",
       "      <td>[u' Drama']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Out of Africa</td>\n",
       "      <td>[u' Biography', u' Drama', u' Romance']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good Will Hunting</td>\n",
       "      <td>[u' Drama']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tootsie</td>\n",
       "      <td>[u' Comedy', u' Drama', u' Romance']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Network</td>\n",
       "      <td>[u' Drama']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Graduate</td>\n",
       "      <td>[u' Comedy', u' Drama', u' Romance']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          title  \\\n",
       "1            Gone with the Wind   \n",
       "1              The Wizard of Oz   \n",
       "1             On the Waterfront   \n",
       "1                     Star Wars   \n",
       "1    E.T. the Extra-Terrestrial   \n",
       "1              Some Like It Hot   \n",
       "1                       Amadeus   \n",
       "1         To Kill a Mockingbird   \n",
       "1   The Best Years of Our Lives   \n",
       "1                  My Fair Lady   \n",
       "1                    Braveheart   \n",
       "1                  The Exorcist   \n",
       "1                   City Lights   \n",
       "1  Mr. Smith Goes to Washington   \n",
       "1                      Rain Man   \n",
       "1                 Out of Africa   \n",
       "1             Good Will Hunting   \n",
       "1                       Tootsie   \n",
       "1                       Network   \n",
       "1                  The Graduate   \n",
       "\n",
       "                                               genre  \n",
       "1                  [u' Drama', u' Romance', u' War']  \n",
       "1  [u' Adventure', u' Family', u' Fantasy', u' Mu...  \n",
       "1                             [u' Crime', u' Drama']  \n",
       "1  [u' Action', u' Adventure', u' Fantasy', u' Sc...  \n",
       "1            [u' Adventure', u' Family', u' Sci-Fi']  \n",
       "1                                       [u' Comedy']  \n",
       "1              [u' Biography', u' Drama', u' Music']  \n",
       "1                                        [u' Drama']  \n",
       "1                  [u' Drama', u' Romance', u' War']  \n",
       "1  [u' Drama', u' Family', u' Musical', u' Romance']  \n",
       "1  [u' Action', u' Biography', u' Drama', u' Hist...  \n",
       "1                                       [u' Horror']  \n",
       "1               [u' Comedy', u' Drama', u' Romance']  \n",
       "1                                        [u' Drama']  \n",
       "1                                        [u' Drama']  \n",
       "1            [u' Biography', u' Drama', u' Romance']  \n",
       "1                                        [u' Drama']  \n",
       "1               [u' Comedy', u' Drama', u' Romance']  \n",
       "1                                        [u' Drama']  \n",
       "1               [u' Comedy', u' Drama', u' Romance']  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame.ix[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = 1 - cosine_similarity(tfidf_matrix)\n",
    "\n",
    "num_clusters = 5\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "km.fit(tfidf_matrix)\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 86,  37,  76,  58,  36, 141, 110,  72, 135,  15,   2, 102, 127,\n",
       "         38,  87,  63,  57, 111,  32, 122, 120,  39,  29,  42,  94, 125,\n",
       "        109,  84, 129, 123,  69, 136,  46,  50, 114, 131, 126,  10,  17,\n",
       "         66,  35,   0,  91, 116, 133, 137,   5,   1, 104,  98,  30,  75,\n",
       "         28, 101,  60,  19,  24, 138,  53,  70, 119,  74,  92,  22, 103,\n",
       "        115,  56,  40, 140,  47, 100,  59,  73,  16, 105,   8,  20, 106,\n",
       "         78,  51, 132, 121,  77, 139,  81,  62,  45,  11,  85, 130,  83,\n",
       "         71,   6,  12,  89, 142,  93,  49,  43,  61,  25,  21,  44, 108,\n",
       "         55,  33,  52,  54,  23,  96,  80,  88,  79,  68,  14,  48,   9,\n",
       "         90, 134,  27, 112, 107,  13,  95, 128,  34, 117,  31,  64,  67,\n",
       "        118,   3,  99,   7,  18,  41,  26,  82, 124,  65,   4, 113,  97],\n",
       "       [ 34, 134,  14,  35, 141, 118,  53,  86,  84,  58,  69, 125, 103,\n",
       "        111,  74, 135,  95,  94,  64,  61,  63, 139,  51, 117,  54,  52,\n",
       "          5,  83,  76,  39,  67,  19, 127,  26,   0,  46,   7, 133,  27,\n",
       "        126, 106,  50,  22,  90, 119,  36,  99,  77, 136,  70, 129,  25,\n",
       "         65,  66,  29, 120,  23, 109,  43, 138,   2,  73,  42,  71,  79,\n",
       "        112,  92,  13,  80, 142, 115,  68,  72,  18, 124,  37,  49, 140,\n",
       "         21,  88,  81,  32,  87, 104, 113,  47, 105,   8,  59,  97,   1,\n",
       "         15,   4,  45,  96, 128,  30,  44,  60,  11,  56,  62, 100,  12,\n",
       "          9,  93,  33,  31, 107,  98,  57,  78,  91, 102,  17,   6,  40,\n",
       "         41,  82,  89, 130, 114, 121,  28, 101,  20, 137,  48,  16,  24,\n",
       "        132,  55, 131, 108,   3,  85, 122, 123, 116,  38, 110,  75,  10],\n",
       "       [134,  80,  61,   2, 128,  60,  22, 127,   4, 113,  19,  92,  76,\n",
       "          5, 129,  48,  65,  36,  99,  21,   3, 136,   6,  23,  81,  63,\n",
       "         71,  31,  38, 114, 101, 125,  46,  86, 103,  87,  41,  53, 115,\n",
       "          8, 126, 118,  39,  28,  16,  59,   1,   9,  52,  82,  62,  58,\n",
       "         69, 109,  24,  35,  11, 120,  91, 124,  42,  10,  43,  78,  66,\n",
       "        105,  68,  51,  33,  44, 135,  84,  32,  75,  95,  67,  50,   0,\n",
       "         98,  74, 104,  79, 139,   7,  72,  12, 142,  15,  49, 112,  96,\n",
       "        121, 132,  40, 123, 130, 111,  47,  89, 106,  73,  45,  85,  94,\n",
       "         55,  97,  37,  26, 138, 131,  17,  27, 107, 110,  88,  57,  93,\n",
       "         90, 119,  64, 140, 108,  20, 137,  25, 116,  54,  14,  56,  13,\n",
       "         34, 117,  77,  29,  83, 100, 122, 133,  18,  30, 102,  70, 141],\n",
       "       [ 13,  99,  53,  35, 137,   9,  22,  64, 112,  54,  81,   4, 126,\n",
       "         76, 119,  67,  52,  39,  15,  58, 117,  27,  97,  73, 107, 103,\n",
       "         86,  79,  63,  74, 100, 104, 129, 136,  19,  11, 101,  37,  87,\n",
       "         43, 133, 114,  61,  33,  78, 110,  34,  82, 135, 141,  90,  65,\n",
       "         89,  85, 105,  88,  96,  93,   5,  29,  48, 108, 109,  40,  91,\n",
       "         95,  83,  71, 125,  21,  12, 116,  66,   6,  69, 122,  60, 131,\n",
       "         25, 132,  32,  72,  18,  42,  10,  62,  55,   3,  50,  31,  84,\n",
       "         56, 113,  47,  46, 130,   7,  28,  23,   0,  70,  59, 106, 140,\n",
       "         20,  57,  17, 115,   8, 121,  24,  38,  30, 142, 127,  80,  75,\n",
       "         98,  41,  68, 139,  94, 111,  51,  77,  45, 120,  44, 118, 102,\n",
       "        134,  36,  16,  92, 123,  49,  26, 124,   2, 128, 138,   1,  14],\n",
       "       [ 83,  35,  53, 126,  77,  54, 140, 137,  63,  87, 108,  76,   9,\n",
       "        104,  43,  14,  18, 142,  69,  28,  10,  72, 139,  73,  27,  66,\n",
       "         34, 127,  44,  37,  39,  57,  42, 125, 122, 118,  19,  64,  85,\n",
       "         86, 102,   8,  79, 134, 100,  29,  94, 136,  89,  31, 106,  65,\n",
       "          1,  23,  95,  68, 119,  20, 133, 117, 107, 138,  40,  33,  45,\n",
       "         59,  32,  12, 112,  17,   5, 128,  55,  15,  70,  52,  47,  81,\n",
       "        124,  30, 109, 135, 105,  50, 103,  75,   0, 141,  97,  46, 121,\n",
       "         22,  49,  93,  26,  88,   2, 129, 101,   4,  56,  38, 116,  78,\n",
       "         67,  24,  41,  74,  61, 131, 123,  80, 130,  98,  84, 132,  21,\n",
       "         99,  82, 120, 110,   6,  62,  90,  51, 113,   7,  92,  91,  25,\n",
       "         13,  36, 115, 111,  11,  16,  96,   3,  71,  58,  48,  60, 114]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(km,  'doc_cluster.pkl')\n",
    "km = joblib.load('doc_cluster.pkl')\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "print()\n",
    "#sort cluster centers by proximity to centroid\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 53,  35,  22, 128,  76,  38, 127,  54,  77,  19,  43, 136,  61,\n",
       "        126, 103,  23,  65,   9,  15,  10,  95,  74,  31, 138, 133,  39,\n",
       "         24, 137,   5,  67,  97,  29, 108, 123,  57,  59, 129, 125,  14,\n",
       "          4,  66, 104,  18,  79,  11,  20,  63,  41, 122,  28,   0,  60,\n",
       "         69,  98,  86,   8,  52, 142,  82,  70,  87, 132,  37,  68,  25,\n",
       "        100,  47,  46, 135, 140,  32,  73, 112,  27,  84, 118, 117, 102,\n",
       "          7,  78,  21, 111, 107, 105,  89, 101, 110,  90,  51,  17,  50,\n",
       "         93, 139,  99,  72,  64,  88,  36, 124, 109, 120,  83, 121,  33,\n",
       "         96,  42,  81,  45,  34,  94, 116,   6,  75,  30, 131, 106,  71,\n",
       "        115,   1,  16,  12, 119, 134,  56,  92,  49, 130,  62, 113,  80,\n",
       "          3,  40,  58,  13, 114,  85,  91,  44,  48,  55, 141,   2,  26],\n",
       "       [  2, 134,  80, 113,  61,   1,   6,  92,  22,  53,  21,  87, 120,\n",
       "          8,  63,  19, 125,  59, 128,  71,  41,   3,  76, 127,   9,  32,\n",
       "         60, 124,  73,  86,  95,  65,   4,   5,  39,  46, 103,  23, 115,\n",
       "        139, 114,  58, 118,  16, 104, 135,  15,  33,  31,  49, 101,  83,\n",
       "         82,  75,  94,  36, 121,   7,  10,  48,  28,  91,  51,  68,  98,\n",
       "         81, 119, 130, 142,  55,  62,  26,  35,  52, 126,  85,  27,  66,\n",
       "        111,  44,  37,  78, 129, 112,  47,  38,  97,  40, 132,  24,  74,\n",
       "         42, 138,  43, 140, 105,  34,  17,  69, 137,  89,  18,  50,  64,\n",
       "        131, 136,  14, 100, 106,  96,  90,   0,  45,  29,  99, 109,  30,\n",
       "        108,  56,  12,  67, 107,  88, 133,  93, 117, 110,  25,  79,  20,\n",
       "         11,  77, 102,  72,  54,  13, 141, 122,  57,  70, 123, 116,  84],\n",
       "       [ 34, 118,  35,  86,  14,  64,  36, 134,  84,  83,  69, 117, 141,\n",
       "        109,  53, 127,  95, 135, 139,   5,  43,  18, 125,  76, 133, 103,\n",
       "         63,  42,  52, 142,  70,  58,  81,  79,  77, 126,  92, 102,  57,\n",
       "          9, 136,  44,  68,  72,  91, 108, 129,  60,  17,  61,  85,  50,\n",
       "         33, 140, 128,  28,  80, 111,  67,  74,  23,  54,  73, 104,  39,\n",
       "         90,  27, 124,  71,  65, 119,  25,  29,  47,  26, 112,  66, 101,\n",
       "         12, 113,  87,   0,  94,  48,  88,  51,   7,  19,   4,  97,  13,\n",
       "         32,  40, 115,   8, 105, 106, 107,  56,   2,  22,  49,  30,  96,\n",
       "         89, 100,  31,  10,  99,  21, 114, 131,  62, 120,  46, 116,  37,\n",
       "         15,  55,  45,  11, 138,  98,   6, 137, 130,  41,  75,  16,   1,\n",
       "         78, 132,  59, 121,  93,  20,  82,  24, 122,   3, 110,  38, 123],\n",
       "       [ 99,  13, 126,  63,  76,   4, 137,  48, 129, 136,  54,  39, 112,\n",
       "         42,   9,  61,  46,  58,  11, 107,  62,  31,   5,  36,  72, 114,\n",
       "         27,  67,  50, 110,  37,  65,  60, 109, 105, 101,  12,  89,  79,\n",
       "         78, 127,  87,  52,  69, 100, 119,  45,  74,  40,  73,  28,   3,\n",
       "        104,  66,  33, 106,  86,  64, 123,   0,  43,  19,  81,  55, 103,\n",
       "         85,  82, 131,  22, 116,  35,  83,  93,  75,  88,  57, 115,  56,\n",
       "        125,  71,  59,  53,  29,  94,  51, 117,  21, 133,  91,  90, 121,\n",
       "        122,  44,  96, 141,  97,  23,  80,  92, 140, 120,  15,  84,  20,\n",
       "          6,   8,  26,  68,  25, 135, 108, 134, 130,   2,  10,  16,  49,\n",
       "         34,  17,  18, 102,  70,  32, 142,  98,  47, 132, 111,  14,  77,\n",
       "        138,   1,  30,  24, 139,  41, 113, 128,   7,  95,  38, 118, 124],\n",
       "       [ 37,  86, 141,  58,  72,  87, 126,  76,  81, 137, 122,  63,  83,\n",
       "         19, 140,  35,  94,  38, 125, 135,  69,  66,  10, 110,  15, 136,\n",
       "         39,  54, 129, 119,  32, 102,  53, 106, 104,  28,  57,  44,  84,\n",
       "        116,  29,  13, 139,  77, 127,  42, 101, 111,  24,  30,  20,  40,\n",
       "        142, 109,  85, 114, 108,   0,  93,  36,  27,  46,   8,  52,   2,\n",
       "        105,  91,  79,  73, 138,  17, 130,  26, 103, 134,  47,  78,  43,\n",
       "         23,  55,  56,  75,  50,  12,  59, 100, 131, 123,  89,   1,   5,\n",
       "         64, 132,   9,  98,  16,  88,  80,   3,  45,  34,  74, 121, 115,\n",
       "         14, 133, 107,  49,  70,  21,  22, 112,  62, 120,  65,  71,  82,\n",
       "         68,  51,  96,  48, 117, 128,  31,  33,  18,   7,   4,  41,  67,\n",
       "         61,  90,  92,  60, 118,  99,  25,  11, 124,   6,  95,  97, 113]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "\n",
      "*** Cluster 0:\n",
      "\n",
      "CENTROID /// The Treasure of the Sierra Madre\n",
      "From Here to Eternity\n",
      "WORDS ///  home /  father /  death /  town /  man /  finally / \n",
      "\n",
      "TITLES ///  The Wizard of Oz /  Vertigo /  Forrest Gump /  The Sound of Music /  E.T. the Extra-Terrestrial /  The Philadelphia Story /  Ben-Hur /  Doctor Zhivago /  The Exorcist /  Mr. Smith Goes to Washington /  Out of Africa /  Good Will Hunting /  Terms of Endearment /  Close Encounters of the Third Kind /  The Graduate /  Wuthering Heights /  Yankee Doodle Dandy / \n",
      "\n",
      "*** Cluster 1:\n",
      "\n",
      "CENTROID /// Schindler's List\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-7b0291c796f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CENTROID /// \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0morder_centroids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0morder_centroids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"WORDS /// \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "print()\n",
    "#sort cluster centers by proximity to centroid\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]     \n",
    "        \n",
    "for i in range(num_clusters):\n",
    "    print(\"*** Cluster %d:\" % i, end='\\n\\n')\n",
    "\n",
    "    print(\"CENTROID /// \", end='')\n",
    "    print(titles[order_centroids[i][0]])\n",
    "    print(titles[order_centroids[i][1]])\n",
    "    \n",
    "    print(\"WORDS /// \", end='')\n",
    "    \n",
    "    for ind in order_centroids[i, :6]: #replace 6 with n words per cluster\n",
    "        print(' %s' % vocab_frame.ix[terms[ind].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore'), end=' / ')\n",
    "    print() #add whitespace\n",
    "    print() #add whitespace\n",
    "    \n",
    "    print(\"TITLES /// \", end='')\n",
    "    for title in frame.ix[i]['title'].values.tolist():\n",
    "        print(' %s / ' % title, end='')\n",
    "    print() #add whitespace\n",
    "    print() #add whitespace\n",
    "    \n",
    "print()\n",
    "print()        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ahora lo limpiamos un poco más: STOPWORDS, STEMMING & TOKENIZING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STOPWORDS\n",
    "\n",
    "# la primera vez hay que descargar la lista de 'stopwords': nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print (stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEMMING\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'cat', u'are', u'run']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# e.g.:\n",
    "tokenize_and_stem('cats are running')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalvocab_stemmed = []\n",
    "\n",
    "for i in synopses:\n",
    "    allwords_stemmed = tokenize_and_stem(i)\n",
    "    totalvocab_stemmed.extend(allwords_stemmed)\n",
    "    \n",
    "vocab_frame = pd.DataFrame({'words': totalvocab_stemmed}, index=totalvocab_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n",
    "                                 min_df=0.2, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(synopses) #fit the vectorizer to synopses\n",
    "\n",
    "terms = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "dist = 1 - cosine_similarity(tfidf_matrix)\n",
    "\n",
    "num_clusters = 5\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "km.fit(tfidf_matrix)\n",
    "clusters = km.labels_.tolist()\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "joblib.dump(km,  'doc_cluster.pkl')\n",
    "km = joblib.load('doc_cluster.pkl')\n",
    "clusters = km.labels_.tolist()\n",
    "\n",
    "films = { 'title': titles, 'synopsis': synopses, 'cluster': clusters, 'genre': genres }\n",
    "frame = pd.DataFrame(films, index = [clusters] , columns = ['title', 'cluster', 'genre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "dist = 1 - cosine_similarity(tfidf_matrix)\n",
    "num_clusters = 5\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "km.fit(tfidf_matrix)\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "\n",
      "*** Cluster 0:\n",
      "\n",
      "WORDS ///  armi /  kill /  soldier /  command /  order /  attack / \n",
      "\n",
      "TITLES ///  Schindler's List /  Lawrence of Arabia /  Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb /  Apocalypse Now /  The Lord of the Rings: The Return of the King /  Patton /  Braveheart /  Platoon /  Dances with Wolves /  All Quiet on the Western Front / \n",
      "\n",
      "*** Cluster 1:\n",
      "\n",
      "WORDS ///  famili /  john /  new /  apart /  york /  father / \n",
      "\n",
      "TITLES ///  The Shawshank Redemption /  Casablanca /  One Flew Over the Cuckoo's Nest /  The Wizard of Oz /  Psycho /  West Side Story /  Star Wars /  The Silence of the Lambs /  The Bridge on the River Kwai /  Gladiator /  From Here to Eternity /  Saving Private Ryan /  Raiders of the Lost Ark /  Jaws /  The Treasure of the Sierra Madre /  The Pianist /  The Deer Hunter /  The African Queen /  Stagecoach /  Mutiny on the Bounty /  The Maltese Falcon /  Rebel Without a Cause /  Rear Window /  The Third Man /  North by Northwest / \n",
      "\n",
      "*** Cluster 2:\n",
      "\n",
      "WORDS ///  home /  return /  love /  marri /  relationship /  children / \n",
      "\n",
      "TITLES ///  Raging Bull /  Citizen Kane /  Sunset Blvd. /  Singin' in the Rain /  Gandhi /  Rocky /  Network /  Yankee Doodle Dandy / \n",
      "\n",
      "*** Cluster 3:\n",
      "\n",
      "WORDS ///  murder /  kill /  polic /  gun /  friend /  arriv / \n",
      "\n",
      "TITLES ///  The Godfather /  Gone with the Wind /  Titanic /  The Godfather: Part II /  Forrest Gump /  The Sound of Music /  It's a Wonderful Life /  Some Like It Hot /  12 Angry Men /  Amadeus /  The Philadelphia Story /  To Kill a Mockingbird /  An American in Paris /  The Best Years of Our Lives /  My Fair Lady /  Ben-Hur /  Doctor Zhivago /  The Good, the Bad and the Ugly /  Goodfellas /  The Exorcist /  City Lights /  The King's Speech /  It Happened One Night /  A Place in the Sun /  Midnight Cowboy /  Mr. Smith Goes to Washington /  Annie Hall /  Out of Africa /  Terms of Endearment /  Tootsie /  Giant /  The Grapes of Wrath /  Close Encounters of the Third Kind /  The Graduate /  Wuthering Heights / \n",
      "\n",
      "*** Cluster 4:\n",
      "\n",
      "WORDS ///  film /  tell /  parti /  girl /  scene /  man / \n",
      "\n",
      "TITLES ///  Vertigo /  On the Waterfront /  E.T. the Extra-Terrestrial /  2001: A Space Odyssey /  Chinatown /  Unforgiven /  A Streetcar Named Desire /  Butch Cassidy and the Sundance Kid /  The Apartment /  High Noon /  The French Connection /  Rain Man /  Good Will Hunting /  Fargo /  Shane /  The Green Mile /  Nashville /  American Graffiti /  Pulp Fiction /  A Clockwork Orange /  Taxi Driver /  Double Indemnity / \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "print()\n",
    "#sort cluster centers by proximity to centroid\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]     \n",
    "        \n",
    "for i in range(num_clusters):\n",
    "    print(\"*** Cluster %d:\" % i, end='\\n\\n')\n",
    "    \n",
    "    print(\"WORDS /// \", end='')\n",
    "    \n",
    "    for ind in order_centroids[i, :6]: #replace 6 with n words per cluster\n",
    "        print(' %s' % vocab_frame.ix[terms[ind].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore'), end=' / ')\n",
    "    print() #add whitespace\n",
    "    print() #add whitespace\n",
    "    \n",
    "    print(\"TITLES /// \", end='')\n",
    "    for title in frame.ix[i]['title'].values.tolist():\n",
    "        print(' %s / ' % title, end='')\n",
    "    print() #add whitespace\n",
    "    print() #add whitespace\n",
    "    \n",
    "print()\n",
    "print()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
